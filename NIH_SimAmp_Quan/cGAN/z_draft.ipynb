{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/users/nguy1432/anaconda3/envs/cGAN/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import speechbrain as sb\n",
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "\n",
    "from models import ResnetGenerator, Discriminator, UnetGenerator, ContextEncoder\n",
    "from models import weights_init_normal\n",
    "from datasets import SpecDataset\n",
    "from utils import normalize_spec, denormalize_spec, db_to_power, power_to_db\n",
    "from utils import plot_waveform, plot_spectrogram, save_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./noise_enhance\t./noise_enhance/cgan_samples_resnet_99\t./noise_enhance/cgan_ckpt_resnet_99\n",
      "./noise_enhance/data/clean_trainset\t./noise_enhance/data/clean_testset\t./noise_enhance/data/noisy_trainset\t./noise_enhance/data/noisy_testset\n",
      "./noise_enhance/data/noisy_trainset/p244_343.wav\t./noise_enhance/data/noisy_testset/p254_213.wav\n"
     ]
    }
   ],
   "source": [
    "class OPT:\n",
    "    cuda_id = 0\n",
    "    my_id = 99\n",
    "    generator_type = 'resnet'\n",
    "    pretrain_dir = './noise_enhance/ckpt_cgan_resnet'\n",
    "    use_context_embed = False\n",
    "    n_steps = 10\n",
    "    batch_size = 32\n",
    "    lr = 0.0001\n",
    "    b1 = 0.5\n",
    "    b2 = 0.999\n",
    "    n_cpu = 1\n",
    "    channels = 1\n",
    "    img_height = 128\n",
    "    img_width = 128\n",
    "    sample_interval = 1000\n",
    "    verbose_interval = 100\n",
    "    num_residual_blocks = 6\n",
    "    gamma = 0.1\n",
    "opt = OPT()\n",
    "\n",
    "dir = './noise_enhance'\n",
    "sample_dir = os.path.join(dir, f'cgan_samples_{opt.generator_type}')\n",
    "ckpt_dir = os.path.join(dir, f'cgan_ckpt_{opt.generator_type}')\n",
    "if opt.my_id is not None:\n",
    "    sample_dir = f'{sample_dir}_{opt.my_id}'\n",
    "    ckpt_dir = f'{ckpt_dir}_{opt.my_id}'\n",
    "clean_train_dir = os.path.join(dir, 'data/clean_trainset')\n",
    "clean_test_dir = os.path.join(dir, 'data/clean_testset')\n",
    "noisy_train_dir = os.path.join(dir, 'data/noisy_trainset')\n",
    "noisy_test_dir = os.path.join(dir, 'data/noisy_testset')\n",
    "train_sample = os.path.join(noisy_train_dir, os.listdir(noisy_train_dir)[0])\n",
    "test_sample = os.path.join(noisy_test_dir, os.listdir(noisy_test_dir)[0])\n",
    "\n",
    "# os.makedirs(sample_dir, exist_ok=True)\n",
    "# os.makedirs(ckpt_dir, exist_ok=True)\n",
    "print(f'{dir}\\t{sample_dir}\\t{ckpt_dir}')\n",
    "print(f'{clean_train_dir}\\t{clean_test_dir}\\t{noisy_train_dir}\\t{noisy_test_dir}')\n",
    "print(f'{train_sample}\\t{test_sample}')\n",
    "\n",
    "img_shape = (opt.channels, opt.img_height, opt.img_width)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "device = f'cuda:{opt.cuda_id}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight\n",
      "Loaded model WEIGHTS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/users/nguy1432/anaconda3/envs/cGAN/lib/python3.8/site-packages/torchaudio/transforms/_transforms.py:611: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize generator and discriminator\n",
    "if opt.use_context_embed:\n",
    "    encoder = ContextEncoder(device).to(device)\n",
    "    context_loss = nn.L1Loss().to(device)\n",
    "if opt.generator_type == 'unet':\n",
    "    generator = UnetGenerator(input_nc=1, output_nc=1, num_downs=7, use_dropout=True).to(device)\n",
    "else:\n",
    "    generator = ResnetGenerator(\n",
    "        input_nc=13 if opt.use_context_embed else 1,\n",
    "        output_nc=1,\n",
    "        ngf=64,\n",
    "        norm_layer=nn.BatchNorm2d,\n",
    "        use_dropout=False,\n",
    "        n_blocks=opt.num_residual_blocks,\n",
    "        padding_type='reflect'\n",
    "    ).to(device)\n",
    "discriminator = Discriminator(img_shape).to(device)\n",
    "\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "print('init weight')\n",
    "if opt.pretrain_dir is not  None:\n",
    "    # generator.load_state_dict(torch.load(os.path.join(opt.pretrain_dir, 'G_model.pt')))\n",
    "    # discriminator.load_state_dict(torch.load(os.path.join(opt.pretrain_dir, 'D_model.pt')))\n",
    "    print('Loaded model WEIGHTS')\n",
    "\n",
    "# Configure data loader\n",
    "dataset = SpecDataset(noisy_dir=noisy_train_dir, gt_dir=clean_train_dir, device=device)\n",
    "dataloader = DataLoader(dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "milestones = [int(opt.n_steps * 0.2), int(opt.n_steps * 0.4), int(opt.n_steps * 0.6), int(opt.n_steps * 0.8)]\n",
    "scheduler_G = torch.optim.lr_scheduler.MultiStepLR(optimizer_G, milestones=milestones, gamma=opt.gamma)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(steps_done, gt=False):\n",
    "    with torch.no_grad():\n",
    "        for s, sample in zip(['test', 'train'], [test_sample, train_sample]):\n",
    "            input_img, input_wav = dataset.getitem_helper(sample)\n",
    "            if gt:\n",
    "                save_spectrogram(input_img[0].detach().cpu().numpy(), os.path.join(sample_dir, os.path.basename(sample).replace('.wav', '.png')))\n",
    "            else:\n",
    "                gen_imgs = generator(input_img.unsqueeze(0).to(device))\n",
    "                gen_imgs = gen_imgs.squeeze(0).data.detach().cpu().numpy()[0] # get 2D\n",
    "                gen_imgs, _ = normalize_spec(gen_imgs)\n",
    "                save_spectrogram(gen_imgs, os.path.join(sample_dir, f\"{s}_step%d.png\" % steps_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 128, 128]) torch.Size([65280])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 203, 1024])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from utils import plot_spectrogram, power_to_db\n",
    "# max_len = 64000 + 64*40\n",
    "# x = dataset[2]\n",
    "# spec, w = x['noisy']\n",
    "# print((203*1024) // (128*512))\n",
    "# w = w[:max_len]\n",
    "# print(spec.shape, w.shape)\n",
    "# emb = encoder(torch.stack([w, w, w], dim=0))\n",
    "# # o = generator(spec.unsqueeze(0))\n",
    "# emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = (opt.n_steps // len(dataloader)) + 1\n",
    "curr_step = 0\n",
    "isContinue = True\n",
    "G_loss = 0.0\n",
    "D_loss = 0.0\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count step\n",
    "curr_step += 1\n",
    "isContinue = curr_step < opt.n_steps\n",
    "# if not isContinue: break\n",
    "#  train model\n",
    "(input_img, input_wav), (gt_img, gt_wav) = batch['noisy'], batch['gt']\n",
    "batch_size = input_img.shape[0]\n",
    "\n",
    "# Adversarial ground truths\n",
    "valid = Variable(Tensor(np.ones((batch_size, *discriminator.output_shape))), requires_grad=False).to(device)\n",
    "fake = Variable(Tensor(np.zeros((batch_size, *discriminator.output_shape))), requires_grad=False).to(device)\n",
    "            \n",
    "# Configure input\n",
    "input_img = Variable(input_img.type(FloatTensor))\n",
    "gt_img = Variable(gt_img.type(FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchaudioTrans import inverse_spectrogram, InverseSpectrogram, InverseMelScale\n",
    "\n",
    "# my_inv = InverseMelScale(\n",
    "#     device=device,\n",
    "#     sample_rate=dataset.sample_rate,\n",
    "#     n_stft=dataset.n_stft,\n",
    "#     n_mels=dataset.n_mels,\n",
    "#     mel_scale='htk',\n",
    "#     max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 203, 1024])\n",
      "torch.Size([32, 13, 128, 128])\n",
      "torch.Size([32, 13, 128, 128]) torch.Size([32, 1, 128, 128]) torch.Size([32, 12, 128, 128]) torch.Size([32, 65280])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    emb = encoder(input_wav) # remove 1 extra dim\n",
    "    print(emb.shape)\n",
    "    batch_size, h, w = emb.shape\n",
    "    xtimes = int((h*w) // (128**2))\n",
    "    emb = emb.view(batch_size, -1)[..., :xtimes*(128**2)]\n",
    "    emb = emb.view(emb.shape[0], -1, 128, 128)\n",
    "    inp = torch.cat([input_img, emb], 1)\n",
    "    print(inp.shape)\n",
    "    gen_imgs = generator(inp)\n",
    "    print(gen_imgs.shape, input_img.shape, emb.shape, input_wav.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 32, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(8, 1, 32, 32)\n",
    "y = torch.rand(8, 1, 32, 32)\n",
    "torch.cat([x, y], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_imgs.shape torch.Size([32, 1, 128, 128])\n",
      "gt_wav.shape torch.Size([32, 1, 65280])\n",
      "cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(spec\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m \u001b[39m# spec_ = inv_transform(spec)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m spec_ \u001b[39m=\u001b[39m my_inv(spec)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(spec_\u001b[39m.\u001b[39mmin())\n\u001b[1;32m     22\u001b[0m \u001b[39m# gen_wavs = grifflim_transform(spec_)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[39m# lim_len = min(gen_wavs.shape[2], gt_wav.shape[2])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m# validity = discriminator(gen_imgs)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# g_loss = adversarial_loss(validity, valid)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/NIH_SimAmp_Quan/cGAN/torchaudioTrans.py:93\u001b[0m, in \u001b[0;36mInverseMelScale.forward\u001b[0;34m(self, melspec)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter):\n\u001b[1;32m     92\u001b[0m     optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 93\u001b[0m     diff \u001b[39m=\u001b[39m melspec \u001b[39m-\u001b[39m specgram\u001b[39m.\u001b[39;49mmatmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfb)\n\u001b[1;32m     94\u001b[0m     new_loss \u001b[39m=\u001b[39m diff\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     95\u001b[0m     \u001b[39m# take sum over mel-frequency then average over other dimensions\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[39m# so that loss threshold is applied par unit timeframe\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "#  Train Generator\n",
    "# -----------------\n",
    "optimizer_G.zero_grad()\n",
    "\n",
    "# Generate a batch of images\n",
    "gen_imgs = generator(input_img)\n",
    "print('gen_imgs.shape', gen_imgs.shape)\n",
    "print('gt_wav.shape', gt_wav.shape)\n",
    "if opt.use_context_embed:\n",
    "    # reconstruct wav\n",
    "    # gen_wavs = dataset.spec_to_audio(gen_imgs)\n",
    "    spec = gen_imgs\n",
    "    spec[0, 0, 0, 0] = -1.\n",
    "    spec = torch.clamp(spec, min=0.0, max=1.0)\n",
    "    spec = denormalize_spec(spec, dataset.spec_min, dataset.spec_max)\n",
    "    spec = db_to_power(spec)\n",
    "    print(spec.device)\n",
    "    # spec_ = inv_transform(spec)\n",
    "    # gen_wavs = grifflim_transform(spec_)\n",
    "\n",
    "    # lim_len = min(gen_wavs.shape[2], gt_wav.shape[2])\n",
    "    # gen_wavs_embed = encoder(gen_wavs.squeeze(1)[..., :lim_len])\n",
    "    # gt_wavs_embed = encoder(gt_wav.squeeze(1)[..., :lim_len])\n",
    "    # print(gen_wavs_embed.shape, gt_wavs_embed.shape)\n",
    "    # ct_loss = context_loss(gen_wavs_embed.view(-1), gt_wavs_embed.view(-1))\n",
    "\n",
    "    # # Loss measures generator's ability to fool the discriminator\n",
    "    # validity = discriminator(gen_imgs)\n",
    "    # g_loss = adversarial_loss(validity, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469, device='cuda:0'),\n",
       " tensor(2.7578, device='cuda:0', grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_img.max(), input_img.min(), gen_imgs.max(), gen_imgs.min())\n",
    "plot_spectrogram(input_img[0][0].detach().clone().cpu())\n",
    "plot_spectrogram(gen_imgs[0][0].detach().clone().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAFT FOR CODE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 65024]) tensor(0.0132, device='cuda:0') tensor(-0.0112, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, x1 = dataset[0], dataset[1]\n",
    "(spec_n0, wav_n0), (spec_c0, wav_c0) = x['noisy'], x['gt']\n",
    "(spec_n1, wav_n1), (spec_c1, wav_c1) = x1['noisy'], x1['gt']\n",
    "\n",
    "wav_n0_ = dataset.spec_to_audio(torch.cat([spec_n0, spec_n1]))\n",
    "print(wav_n0_.shape, wav_n0_.max(), wav_n0_.min())\n",
    "# plot_waveform(wav_n0.cpu(), sr=dataset.sample_rate)\n",
    "# plot_waveform(wav_n0_.cpu(), sr=dataset.sample_rate)\n",
    "# Audio(wav_n0_.cpu(), rate=dataset.sample_rate, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_minmax_spectrogram:   0%|          | 0/11000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_minmax_spectrogram: 100%|██████████| 11000/11000 [00:11<00:00, 995.01it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-96.6628, device='cuda:0'), tensor(6.0352, device='cuda:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.is_normal_spec = False\n",
    "min_val, max_val = dataset.get_minmax_spec(True)\n",
    "dataset.is_normal_spec = True\n",
    "min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_norm = dataset.normalize(x).detach().clone().cpu().numpy()[0]\n",
    "x1 = dataset[4]\n",
    "print(x1['noisy'][0].max(), x1['noisy'][0].min())\n",
    "plot_spectrogram(x1['noisy'][0].detach().clone().cpu().numpy()[0])\n",
    "plot_spectrogram(x1['gt'][0].detach().clone().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, torch.Size([8, 1, 128, 128]), torch.Size([8, 1, 128, 128]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    break\n",
    "input_img, gt_img = batch['noisy'][0], batch['gt'][0]\n",
    "batch_size = input_img.shape[0]\n",
    "batch_size, input_img.shape, gt_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid.shape torch.Size([8, 1, 8, 8])\n",
      "fake.shape torch.Size([8, 1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "valid = Variable(Tensor(np.ones((batch_size, *discriminator.output_shape))), requires_grad=False)\n",
    "fake = Variable(Tensor(np.zeros((batch_size, *discriminator.output_shape))), requires_grad=False)\n",
    "print('valid.shape', valid.shape)\n",
    "print('fake.shape', fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_img.shape torch.Size([8, 1, 128, 128])\n",
      "gt_img.shape torch.Size([8, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Configure input\n",
    "input_img = Variable(input_img.type(FloatTensor))\n",
    "gt_img = Variable(gt_img.type(FloatTensor))\n",
    "print('input_img.shape', input_img.shape)\n",
    "print('gt_img.shape', gt_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_imgs.shape torch.Size([1, 1, 96, 128])\n"
     ]
    }
   ],
   "source": [
    "optimizer_G.zero_grad()\n",
    "# Generate a batch of images\n",
    "# gen_imgs = generator(input_img)\n",
    "x = torch.rand(1, 1, 96, 128).to(device)\n",
    "gen_imgs = generator(x)\n",
    "print('gen_imgs.shape', gen_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validity.shape torch.Size([1, 1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Loss measures generator's ability to fool the discriminator\n",
    "validity = discriminator(gen_imgs)\n",
    "print('validity.shape', validity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/users/nguy1432/anaconda3/envs/cGAN/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([8, 1, 8, 8])) that is different to the input size (torch.Size([1, 1, 8, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "g_loss = adversarial_loss(validity, valid)\n",
    "g_loss.backward()\n",
    "optimizer_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validity_real.shape torch.Size([8, 1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#  Train Discriminator\n",
    "# ---------------------\n",
    "\n",
    "optimizer_D.zero_grad()\n",
    "\n",
    "# Loss for real images\n",
    "validity_real = discriminator(gt_img)\n",
    "d_real_loss = adversarial_loss(validity_real, valid)\n",
    "print('validity_real.shape', validity_real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss for fake images\n",
    "validity_fake = discriminator(gen_imgs.detach())\n",
    "d_fake_loss = adversarial_loss(validity_fake, fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total discriminator loss\n",
    "d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "d_loss.backward()\n",
    "optimizer_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(steps_done):\n",
    "    # Sample from test set\n",
    "    input_img = dataset.getitem_helper(test_sample)\n",
    "    gen_imgs = generator(input_img.unsqueeze(0).to(device))\n",
    "    # save_image(gen_imgs.squeeze(0).data, f\"./{sample_dir}/test_step%d.png\" % steps_done, normalize=True)\n",
    "    # Sample from train set\n",
    "    input_img = dataset.getitem_helper(train_sample)\n",
    "    gen_imgs = generator(input_img.unsqueeze(0).to(device))\n",
    "    # save_image(gen_imgs.squeeze(0).data, f\"./{sample_dir}/train_step%d.png\" % steps_done, normalize=True)\n",
    "sample_image(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_mnist = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"./data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(128), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "for batch in dataloader_mnist:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
